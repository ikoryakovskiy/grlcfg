experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 0
  steps: 7000000
  rate: 0
  test_interval: 10
  output: rbdl_nmpc_2ac_tc_squat_fb_sl_fa_vc
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/leo_squatting
      control_step: 0.03
      integration_steps: 25
      dynamics: 
        type: dynamics/rbdl
        file: leo_vc/leo_fb_sl.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      target_dof: 4
      animation: immediate
      condition: [0.01, 0.01]
      lower_height: 0.28
      upper_height: 0.35
      mode: vc
    task: 
      type: task/leo_squatting
      timeout: 10
      randomize: 0
      weight_nmpc: 0.0001
      weight_nmpc_aux: 1.0
      weight_nmpc_qd: 1.0
      weight_shaping: 0.0
      power: 2.0
      setpoint_reward: 0
      continue_after_fall: 0
  agent: 
    type: agent/master/selective
    agent1: 
      type: agent/sub/compartmentalized
      min: [ -100, -100, -100, -100, -100, -100, 0.34 ]
      max: [ 100, 100, 100, 100, 100, 100, 0.36 ]
      agent: 
        type: agent/fixed
        policy: 
          type: mapping/policy/parameterized/pid
          setpoint: [ 0.588085,   -1.325015,    0.899765,   0,   0,    0 ]
          outputs: experiment/environment/task/action_dims
          p: [ 10, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0 ]
          i: [  ]
          d: [  ]
          il: [  ]
          action_min: experiment/environment/task/action_min
          action_max: experiment/environment/task/action_max
    agent2: 
      type: agent/sub/compartmentalized
      min: [ -100, -100, -100, -100, -100, -100, 0.27 ]
      max: [ 100, 100, 100, 100, 100, 100, 0.29 ]
      agent: experiment/agent/agent1/agent
  test_agent: experiment/agent
  save_every: never
