experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 1
  steps: 0
  rate: 33.333
  test_interval: -1
  output: complex_rbdl_mlrti_2dpg_ou_squat_fb_sl_fa_vc_mef
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/leo_squatting
      control_step: 0.03
      integration_steps: 25
      target_dof: 4
      dynamics: 
        type: dynamics/rbdl
        file: leo_vc/leo_fb_sl_real.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      true_model: 
        type: model/dynamical
        control_step: experiment/environment/model/control_step
        integration_steps: 5
        dynamics: 
          type: dynamics/rbdl
          file: leo_vc/leo_fb_sl.lua
      animation: nope
      lower_height: 0.28
      upper_height: 0.35
      precision: [0.01, 0.01]
      mode: vc
      sim_filtered: 0
      sub_true_action: 
        type: signal/vector
      sub_sma_state: 
        type: signal/vector
      timer_switch: 0
      idle_time: 0
    task: 
      type: task/leo_squatting
      target_env: experiment/environment/model/target_env
      timeout: 10000
      randomize: 0
      weight_nmpc: 0.0001
      weight_nmpc_aux: 1
      weight_nmpc_qd: 1
      weight_shaping: 0
      power: 2
      use_mef: 1
      setpoint_reward: 0
      continue_after_fall: 1
      gamma: 0.97
      fixed_arm: 1
    exporter: 
      type: exporter/csv
      file: complex_rbdl_mlrti_2dpg_ou_squat_fb_sl_fa_vc_mef
      fields: time, state, observation, action, reward, terminal
      style: meshup
      variant: all
      precision: 6
      enabled: 1
  agent: 
    type: agent/leo/sma
    environment: experiment/environment
    main_steps: 200000
    main_timeout: 15.0
    test_interval: 10
    output: complex_rbdl_mlrti_2dpg_ou_squat_fb_sl_fa_vc_mef_main
    action_min: experiment/environment/task/action_min
    action_max: experiment/environment/task/action_max
    agent_prepare: 
      type: agent/master/selective
      agent1: 
        type: agent/sub/compartmentalized
        min: [0.8, -100, -100, -100, -100, -100, -100, -100]
        max: [100, -2.0, 100, 100, 100, 100, 100, 100]
        agent: 
          type: agent/fixed
          policy: 
            type: mapping/policy/parameterized/pid
            setpoint: [1.1, -2.13, 1.27, 0, 0, 0, 0, 0]
            outputs: experiment/environment/task/action_dims
            p: [45, 0, 0, 0, 0, 0, 0, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 40, 0, 0, 0, 0, 0]
            i: []
            d: []
            il: []
            action_min: experiment/environment/task/action_min
            action_max: experiment/environment/task/action_max
      agent2: 
        type: agent/sub/compartmentalized
        min: []
        max: []
        agent: 
          type: agent/fixed
          policy: 
            type: mapping/policy/parameterized/pid
            setpoint: [1.1, -2.13, 1.27, 0, 0, 0, 0, 0]
            outputs: experiment/environment/task/action_dims
            p: [12, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0]
            i: []
            d: []
            il: []
            action_min: experiment/environment/task/action_min
            action_max: experiment/environment/task/action_max
    agent_standup: 
      type: agent/leo/fixed
      policy: 
        type: mapping/policy/parameterized/pid
        setpoint: [1.1, -2.13, 1.27, 0, 0, 0, 0, 0]
        outputs: experiment/environment/task/action_dims
        p: []
        i: []
        d: []
        il: []
        action_min: experiment/environment/task/action_min
        action_max: experiment/environment/task/action_max
    agent_main: 
      type: agent/master/sequential/additive
      agent1: 
        type: agent/fixed
        policy: 
          type: mapping/policy/post/lowpass
          order: 0
          sampling: 33.333333333
          cutoff: 5
          action_dims: experiment/environment/task/action_dims
          magnitude: [1, 0, 0, 0]
          policy: 
            type: mapping/policy/nmpc_mlrti
            verbose: 0
            initFeedback: 1
            action_min: experiment/environment/task/action_min
            action_max: experiment/environment/task/action_max
            lua_model: leo_fb_sl_fa.lua
            model_name: nmpc_leo_squat_fb_sl_fa_vc
            nmpc_model_name: nmpc_leo_squat_fb_sl_fa_vc
            ninit: 0
      agent2: 
        type: agent/master/selective
        agent1: 
          type: agent/sub/compartmentalized
          min: [-100, -100, -100, -100, -100, -100, -100, 0.34]
          max: [100, 100, 100, 100, 100, 100, 100, 0.36]
          agent: 
            type: agent/td
            policy: 
              type: mapping/policy/post/noise
              sigma: [0.01]
              theta: [0.01]
              output_min: experiment/environment/task/action_min
              output_max: experiment/environment/task/action_max
              policy: 
                type: mapping/policy/action
                sigma: []
                output_min: experiment/environment/task/action_min
                output_max: experiment/environment/task/action_max
                projector: 
                  type: projector/tile_coding
                  tilings: 16
                  memory: 8388608
                  safe: 1
                  resolution: [0.2, 0.2, 0.2, 1, 1, 0.5, 10, 0.01]
                  wrapping: [0, 0, 0, 0, 0, 0, 0, 0]
                representation: 
                  type: representation/parameterized/linear
                  init_min: [0]
                  init_max: [0]
                  memory: experiment/agent/agent_main/agent2/agent1/agent/policy/policy/projector/memory
                  outputs: experiment/environment/task/action_dims
                  output_min: experiment/environment/task/action_min
                  output_max: experiment/environment/task/action_max
            predictor: 
              type: predictor/dpg
              alpha: 0.06
              beta_v: 0.1
              beta_a: 0.01
              gamma: experiment/environment/task/gamma
              lambda: 0.65
              projector: experiment/agent/agent_main/agent2/agent1/agent/policy/policy/projector
              critic_representation: 
                type: representation/parameterized/linear
                init_min: [0]
                init_max: [0]
                memory: experiment/agent/agent_main/agent2/agent1/agent/policy/policy/projector/memory
                outputs: 1
                output_min: []
                output_max: []
              critic_trace: 
                type: trace/enumerated/replacing
              advantage_representation: 
                type: representation/parameterized/linear
                init_min: [0]
                init_max: [0]
                memory: experiment/agent/agent_main/agent2/agent1/agent/policy/policy/projector/memory
                outputs: experiment/environment/task/action_dims
                output_min: []
                output_max: []
              actor_representation: experiment/agent/agent_main/agent2/agent1/agent/policy/policy/representation
        agent2: 
          type: agent/sub/compartmentalized
          min: [-100, -100, -100, -100, -100, -100, -100, 0.27]
          max: [100, 100, 100, 100, 100, 100, 100, 0.29]
          agent: experiment/agent/agent_main/agent2/agent1/agent
      pub_action1: experiment/environment/model/sub_true_action
      exporter: 
        type: exporter/csv
        file: complex_rbdl_mlrti_2dpg_ou_squat_fb_sl_fa_vc_mef_elements
        style: meshup
        variant: all
        precision: 6
        enabled: 1
      output_min: experiment/environment/task/action_min
      output_max: experiment/environment/task/action_max
    agent_test: 
      type: agent/master/sequential/additive
      agent1: experiment/agent/agent_main/agent1
      agent2: 
        type: agent/master/selective
        agent1: 
          type: agent/sub/compartmentalized
          min: experiment/agent/agent_main/agent2/agent1/min
          max: experiment/agent/agent_main/agent2/agent1/max
          agent: 
            type: agent/td
            policy: 
              type: mapping/policy/post/noise
              sigma: []
              theta: []
              output_min: experiment/environment/task/action_min
              output_max: experiment/environment/task/action_max
              policy: experiment/agent/agent_main/agent2/agent1/agent/policy/policy
            predictor: experiment/agent/agent_main/agent2/agent1/agent/predictor
        agent2: 
          type: agent/sub/compartmentalized
          min: experiment/agent/agent_main/agent2/agent2/min
          max: experiment/agent/agent_main/agent2/agent2/max
          agent: experiment/agent/agent_test/agent2/agent1/agent
      pub_action1: experiment/environment/model/sub_true_action
      exporter: experiment/agent/agent_main/exporter
      output_min: experiment/environment/task/action_min
      output_max: experiment/environment/task/action_max
    upright_trigger: 
      type: trigger
      min: [0.8, -2.4, 1.0, -100, -100, -100, -100, -100]
      max: [1.3, -1.8, 1.6, 100, 100, 100, 100, 100]
      delay: 5
    feet_on_trigger: 
      type: trigger
      min: [1]
      max: [1]
      delay: 3
    pub_sma_state: experiment/environment/model/sub_sma_state
  save_every: never
