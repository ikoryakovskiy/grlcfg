experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 0
  steps: 70000
  rate: 0
  test_interval: 10
  output: leo_nmpc_ac_tc_squat_fb_sl_fa
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/dynamical
      control_step: 0.03
      integration_steps: 25
      dynamics: 
        type: dynamics/rbdl_leo
        file: leo_fb_sl.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      dof_count: 4
      target_env: 
        type: environment/communicator
        converter: 
          type: converter/state_action_converter
          state_in: svTorsoAngle, svTorsoAngleRate, svLeftArmAngle, svLeftArmAngleRate, svRightHipAngle, svRightHipAngleRate, svLeftHipAngle, svLeftHipAngleRate, svRightKneeAngle, svRightKneeAngleRate, svLeftKneeAngle, svLeftKneeAngleRate, svRightAnkleAngle, svRightAnkleAngleRate, svLeftAnkleAngle, svLeftAnkleAngleRate, svRightToeContact, svRightHeelContact, svLeftToeContact, svLeftHeelContact
          state_out: svRightAnkleAngle, svRightKneeAngle, svRightHipAngle, svLeftArmAngle, svRightAnkleAngleRate, svRightKneeAngleRate, svRightHipAngleRate, svLeftArmAngleRate
          action_in: avRightAnkleTorque, avRightKneeTorque, avRightHipTorque, avLeftArmTorque
          action_out: avLeftArmTorque, avRightHipTorque, avRightHipTorque, avRightKneeTorque, avRightKneeTorque, avRightAnkleTorque, avRightAnkleTorque
        communicator: 
          type: communicator/zeromq
          pub: tcp://*:5561
          sub: tcp://192.168.1.10:5562
          event: tcp://192.168.1.10:5560
          event_mode: ZMQ_SYNC_SUB
        target_obs_dims: 20
        target_action_dims: 7
      action_min: [ -10.7, -10.7, -10.7 ]
      action_max: [ 10.7,  10.7,  10.7 ]
    task: 
      type: task/leoSquatFA
      timeout: 0
      initial_setpoint: 0.28
      rand_init: 0
    exporter: 
      type: exporter/csv
      file: leo_nmpc_ac_tc_squat_fb_sl_fa
      fields: time, state, observation, action, reward, terminal
      style: meshup
      variant: test
  agent: 
    type: agent/leo_squatting_agent
    agent_standup: 
      type: agent/fixed
      policy: 
        type: policy/parameterized/pid
        setpoint: [1.6, 1.6, -1.9, -1.9, 0.85, 0.85, 0, 0, 0, 0, 0, 0]
        outputs: 6
        p: [5.5, 0, 0, 0, 0, 0, 0, 5.5, 0, 0, 0, 0, 0, 0, 6.1, 0, 0, 0, 0, 0, 0, 6.1, 0, 0, 0, 0, 0, 0, 7.35, 0, 0, 0, 0, 0, 0, 7.35, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2.1, 0, 0, 0, 0, 0, 0, 2.1, 0, 0, 0, 0, 0, 0, 2.06, 0, 0, 0, 0, 0, 0, 2.06]
        i: []
        d: []
        il: []
    agent_learn: 
      type: agent/master/sequential/additive
      agent1: 
        type: agent/fixed
        policy: 
          type: policy/nmpc
          action_min: experiment/environment/task/action_min
          action_max: experiment/environment/task/action_max
          lua_model: leo_fb_sl_fa.lua
          model_name: nmpc_leo_squat_fb_sl_fa
          nmpc_model_name: nmpc_leo_squat_fb_sl_fa
          outputs: 3
          initFeedback: 1
          verbose: 0
      agent2: 
        type: agent/td
        policy: 
          type: policy/action
          sigma: [1, 1, 1]
          use_ou: 0
          theta: []
          center: []
          output_min: experiment/environment/task/action_min
          output_max: experiment/environment/task/action_max
          projector: 
            type: projector/pre/peaked
            peaking: [0, 0, 0, 0, 0, 0, 0]
            input_min: experiment/environment/task/observation_min
            input_max: experiment/environment/task/observation_max
            projector: 
              type: projector/tile_coding
              tilings: 16
              memory: 25165824
              safe: 1
              resolution: [0.28, 0.28, 0.28, 10, 10, 10, 0.01]
              wrapping: [0, 0, 0, 0, 0, 0, 0]
          representation: 
            type: representation/parameterized/linear
            init_min: [0]
            init_max: [1]
            memory: experiment/agent/agent2/policy/projector/projector/memory
            outputs: experiment/environment/task/action_dims
            output_min: [-3, -3, -3]
            output_max: [3, 3, 3]
        predictor: 
          type: predictor/ac/action
          alpha: 0.2
          beta: 0.02
          gamma: 0.9962
          lambda: 0.8582
          critic_projector: 
            type: projector/pre/peaked
            peaking: experiment/agent/agent2/policy/projector/peaking
            input_min: experiment/environment/task/observation_min
            input_max: experiment/environment/task/observation_max
            projector: 
              type: projector/tile_coding
              tilings: experiment/agent/agent2/policy/projector/projector/tilings
              memory: experiment/agent/agent2/policy/projector/projector/memory
              safe: experiment/agent/agent2/policy/projector/projector/safe
              resolution: experiment/agent/agent2/policy/projector/projector/resolution
              wrapping: experiment/agent/agent2/policy/projector/projector/wrapping
          critic_representation: 
            type: representation/parameterized/linear
            init_min: [0]
            init_max: [1]
            memory: experiment/agent/agent2/policy/projector/projector/memory
            outputs: 1
            output_min: []
            output_max: []
          critic_trace: 
            type: trace/enumerated/replacing
          actor_projector: experiment/agent/agent2/policy/projector
          actor_representation: experiment/agent/agent2/policy/representation
      output_min: experiment/environment/task/action_min
      output_max: experiment/environment/task/action_max
  test_agent: 
    type: agent/master/sequential/additive
    agent1: experiment/agent/agent1
    agent2: 
      type: agent/fixed
      policy: 
        type: policy/action
        sigma: []
        use_ou: 0
        theta: []
        center: []
        output_min: experiment/environment/task/action_min
        output_max: experiment/environment/task/action_max
        projector: experiment/agent/agent2/policy/projector
        representation: experiment/agent/agent2/policy/representation
    output_min: experiment/environment/task/action_min
    output_max: experiment/environment/task/action_max
  save_every: never
