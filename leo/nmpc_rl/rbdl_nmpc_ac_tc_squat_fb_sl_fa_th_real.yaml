experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 0
  steps: 70000
  rate: 30
  test_interval: 10
  output: rbdl_nmpc_ac_tc_squat_fb_sl_fa_th_real
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/leo_squatting
      control_step: 0.03
      integration_steps: 25
      dynamics: 
        type: dynamics/rbdl
        file: leo_tc/leo_fb_sl_real.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      target_dof: 4
      animation: nope
      lower_height: 0.28
      upper_height: 0.35
    task: 
      type: task/leo_squatting
      timeout: 10
      rand_init: 0
  agent: 
    type: agent/master/sequential/additive
    agent1: 
      type: agent/fixed
      policy: 
        type: mapping/policy/nmpc
        verbose: 0
        initFeedback: 1
        action_min: experiment/environment/task/action_min
        action_max: experiment/environment/task/action_max
        lua_model: leo_fb_sl_fa.lua
        model_name: nmpc_leo_squat_fb_sl_fa
        nmpc_model_name: nmpc_leo_squat_fb_sl_fa
        feedback: threaded
        n_iter: 1
    agent2: 
      type: agent/td
      policy: 
        type: mapping/policy/action
        sigma: [1, 1, 1]
        output_min: experiment/environment/task/action_min
        output_max: experiment/environment/task/action_max
        projector: 
          type: projector/pre/peaked
          peaking: [0, 0, 0, 0, 0, 0, 0]
          input_min: experiment/environment/task/observation_min
          input_max: experiment/environment/task/observation_max
          projector: 
            type: projector/tile_coding
            tilings: 16
            memory: 25165824
            safe: 1
            resolution: [0.28, 0.28, 0.28, 10, 10, 10, 0.01]
            wrapping: [0, 0, 0, 0, 0, 0, 0]
        representation: 
          type: representation/parameterized/linear
          init_min: [0]
          init_max: [1]
          memory: experiment/agent/agent2/policy/projector/projector/memory
          outputs: experiment/environment/task/action_dims
          output_min: experiment/environment/task/action_min
          output_max: experiment/environment/task/action_max
      predictor: 
        type: predictor/ac/action
        alpha: 0.2
        beta: 0.02
        gamma: 0.9962
        lambda: 0.8582
        update_method: proportional
        step_limit: [  ]
        critic_projector: 
          type: projector/pre/peaked
          peaking: experiment/agent/agent2/policy/projector/peaking
          input_min: experiment/environment/task/observation_min
          input_max: experiment/environment/task/observation_max
          projector: 
            type: projector/tile_coding
            tilings: experiment/agent/agent2/policy/projector/projector/tilings
            memory: experiment/agent/agent2/policy/projector/projector/memory
            safe: experiment/agent/agent2/policy/projector/projector/safe
            resolution: experiment/agent/agent2/policy/projector/projector/resolution
            wrapping: experiment/agent/agent2/policy/projector/projector/wrapping
        critic_representation: 
          type: representation/parameterized/linear
          init_min: [0]
          init_max: [1]
          memory: experiment/agent/agent2/policy/projector/projector/memory
          outputs: 1
          output_min: []
          output_max: []
        critic_trace: 
          type: trace/enumerated/replacing
        actor_projector: experiment/agent/agent2/policy/projector
        actor_representation: experiment/agent/agent2/policy/representation
    output_min: experiment/environment/task/action_min
    output_max: experiment/environment/task/action_max
  test_agent: 
    type: agent/master/sequential/additive
    agent1: experiment/agent/agent1
    agent2: 
      type: agent/fixed
      policy: 
        type: mapping/policy/action
        sigma: []
        output_min: experiment/environment/task/action_min
        output_max: experiment/environment/task/action_max
        projector: experiment/agent/agent2/policy/projector
        representation: experiment/agent/agent2/policy/representation
    output_min: experiment/environment/task/action_min
    output_max: experiment/environment/task/action_max
  save_every: run
