experiment: 
  type: experiment/online_learning
  runs: 1
  trials: 1
  steps: 0
  rate: 0
  test_interval: 0
  output: rbdl_nmpc_2ac_tc_squat_fb_sl_fa_play
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/leo_squatting
      control_step: 0.03
      integration_steps: 25
      dynamics: 
        type: dynamics/rbdl
        file: leo_tc/leo_fb_sl.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      target_dof: 4
      animation: immediate
      condition: [0.01, 0.01]
      lower_height: 0.28
      upper_height: 0.35
    task: 
      type: task/leo_squatting
      timeout: 10
      randomize: 0
      weight_nmpc: 0.0001
      weight_nmpc_aux: 1.0
      weight_shaping: 0.0
      power: 2.0
      setpoint_reward: 0
      gamma: 0
      continue_after_fall: 0
  agent: 
    type: agent/master/sequential/additive
    agent1: 
      type: agent/fixed
      policy: 
        type: mapping/policy/nmpc
        verbose: 0
        initFeedback: 1
        action_min: experiment/environment/task/action_min
        action_max: experiment/environment/task/action_max
        lua_model: leo_fb_sl_fa.lua
        model_name: nmpc_leo_squat_fb_sl_fa
        nmpc_model_name: nmpc_leo_squat_fb_sl_fa
        feedback: non-threaded
        n_iter: 1
    agent2: 
      type: agent/master/selective
      agent1: 
        type: agent/sub/compartmentalized
        min: [-100, -100, -100, -100, -100, -100, 0.34]
        max: [100, 100, 100, 100, 100, 100, 0.36]
        agent: 
          type: agent/td
          policy: 
            type: mapping/policy/action
            sigma: [1, 1, 1]
            output_min: experiment/environment/task/action_min
            output_max: experiment/environment/task/action_max
            projector: 
              type: projector/pre/peaked
              peaking: [0, 0, 0, 0, 0, 0, 0]
              input_min: experiment/environment/task/observation_min
              input_max: experiment/environment/task/observation_max
              projector: 
                type: projector/tile_coding
                tilings: 16
                memory: 25165824
                safe: 1
                resolution: [0.28, 0.28, 0.28, 10, 10, 10, 0.01]
                wrapping: [0, 0, 0, 0, 0, 0, 0]
            representation: 
              type: representation/parameterized/linear
              init_min: [0]
              init_max: [1]
              memory: experiment/agent/agent2/agent1/agent/policy/projector/projector/memory
              outputs: experiment/environment/task/action_dims
              output_min: experiment/environment/task/action_min
              output_max: experiment/environment/task/action_max
          predictor: 
            type: predictor/ac/action
            alpha: 0.2
            beta: 0.02
            gamma: 0.9962
            lambda: 0.8582
            update_method: proportional
            step_limit: []
            critic_projector: 
              type: projector/pre/peaked
              peaking: experiment/agent/agent2/agent1/agent/policy/projector/peaking
              input_min: experiment/environment/task/observation_min
              input_max: experiment/environment/task/observation_max
              projector: 
                type: projector/tile_coding
                tilings: experiment/agent/agent2/agent1/agent/policy/projector/projector/tilings
                memory: experiment/agent/agent2/agent1/agent/policy/projector/projector/memory
                safe: experiment/agent/agent2/agent1/agent/policy/projector/projector/safe
                resolution: experiment/agent/agent2/agent1/agent/policy/projector/projector/resolution
                wrapping: experiment/agent/agent2/agent1/agent/policy/projector/projector/wrapping
            critic_representation: 
              type: representation/parameterized/linear
              init_min: [0]
              init_max: [1]
              memory: experiment/agent/agent2/agent1/agent/policy/projector/projector/memory
              outputs: 1
              output_min: []
              output_max: []
            critic_trace: 
              type: trace/enumerated/replacing
            actor_projector: experiment/agent/agent2/agent1/agent/policy/projector
            actor_representation: experiment/agent/agent2/agent1/agent/policy/representation
      agent2: 
        type: agent/sub/compartmentalized
        min: [-100, -100, -100, -100, -100, -100, 0.27]
        max: [100, 100, 100, 100, 100, 100, 0.29]
        agent: experiment/agent/agent2/agent1/agent
    exporter: 
      type: exporter/csv
      file: additive_rbdl_nmpc_2ac_tc_squat_fb_sl_fa_play
      style: meshup
      variant: all
      enabled: 1
    output_min: experiment/environment/task/action_min
    output_max: experiment/environment/task/action_max
  test_agent: 
    type: agent/master/sequential/additive
    agent1: experiment/agent/agent1
    agent2: 
      type: agent/master/selective
      agent1: 
        type: agent/sub/compartmentalized
        min: experiment/agent/agent2/agent1/min
        max: experiment/agent/agent2/agent1/max
        agent: 
          type: agent/fixed
          policy: 
            type: mapping/policy/action
            sigma: []
            output_min: experiment/environment/task/action_min
            output_max: experiment/environment/task/action_max
            projector: experiment/agent/agent2/agent1/agent/policy/projector
            representation: experiment/agent/agent2/agent1/agent/policy/representation
      agent2: 
        type: agent/sub/compartmentalized
        min: experiment/agent/agent2/agent2/min
        max: experiment/agent/agent2/agent2/max
        agent: experiment/test_agent/agent2/agent1/agent
    exporter: experiment/agent/exporter
    output_min: experiment/environment/task/action_min
    output_max: experiment/environment/task/action_max
  load_file: leo_nmpc_2rl_rbdl_nmpc_2ac_tc_squat_fb_sl_fa-00100-00001-10000-00000-00000-mp0-run0
  save_every: never
