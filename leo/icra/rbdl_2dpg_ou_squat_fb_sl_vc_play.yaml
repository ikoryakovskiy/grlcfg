experiment: 
  type: experiment/leo/online_learning/squatting
  runs: 1
  trials: 1
  steps: 0
  rate: 0
  test_interval: 0
  output: rbdl_2dpg_ou_squat_fb_sl_vc_play
  environment: 
    type: environment/sandbox
    model: 
      type: sandbox_model/leo_squatting
      control_step: 0.03
      integration_steps: 25
      target_dof: 4
      dynamics: 
        type: dynamics/rbdl
        file: leo_vc/leo_fb_sl.lua
        points: tip_left, heel_left, root
        auxiliary: mm, com, comv, am
      animation: full
      lower_height: 0.28
      upper_height: 0.35
      precision: [0.01, 0.01]
      mode: vc
      sim_filtered: 0
      timer_switch: 0
      idle_time: 0
    task: 
      type: task/leo_squatting
      timeout: 15
      randomize: 0.01
      weight_nmpc: 0.001
      weight_nmpc_aux: 1.0
      weight_nmpc_qd: 1.0
      weight_shaping: 0.0
      power: 2.0
      use_mef: 0
      setpoint_reward: 0
      continue_after_fall: 0
      gamma: 0.97
      fixed_arm: 0
      lower_height: experiment/environment/model/lower_height
      upper_height: experiment/environment/model/upper_height
      friction_compensation: 0
  agent: 
    type: agent/master/selective
    agent1: 
      type: agent/sub/compartmentalized
      min: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 0.34]
      max: [100, 100, 100, 100, 100, 100, 100, 100, 100, 0.36]
      agent: 
        type: agent/td
        policy: 
          type: mapping/policy/post/noise
          sigma: [0.005]
          theta: [0.5]
          output_min: experiment/environment/task/action_min
          output_max: experiment/environment/task/action_max
          policy: 
            type: mapping/policy/action
            sigma: []
            output_min: experiment/environment/task/action_min
            output_max: experiment/environment/task/action_max
            projector: 
              type: projector/tile_coding
              tilings: 16
              memory: 8388608
              safe: 1
              resolution: [0.28, 0.28, 0.28, 0.28, 10, 10, 10, 10, 10, 0.01]
              wrapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
            representation: 
              type: representation/parameterized/linear
              init_min: [0]
              init_max: [0]
              memory: experiment/agent/agent1/agent/policy/policy/projector/memory
              outputs: experiment/environment/task/action_dims
              output_min: experiment/environment/task/action_min
              output_max: experiment/environment/task/action_max
        predictor: 
          type: predictor/dpg
          alpha: 0.01
          beta_v: 0.1
          beta_a: 0.01
          gamma: experiment/environment/task/gamma
          lambda: 0.65
          projector: experiment/agent/agent1/agent/policy/policy/projector
          critic_representation: 
            type: representation/parameterized/linear
            init_min: [0]
            init_max: [0]
            memory: experiment/agent/agent1/agent/policy/policy/projector/memory
            outputs: 1
            output_min: []
            output_max: []
          critic_trace: 
            type: trace/enumerated/replacing
          advantage_representation: 
            type: representation/parameterized/linear
            init_min: [0]
            init_max: [0]
            memory: experiment/agent/agent1/agent/policy/policy/projector/memory
            outputs: experiment/environment/task/action_dims
            output_min: []
            output_max: []
          actor_representation: experiment/agent/agent1/agent/policy/policy/representation
    agent2: 
      type: agent/sub/compartmentalized
      min: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 0.27]
      max: [100, 100, 100, 100, 100, 100, 100, 100, 100, 0.29]
      agent: experiment/agent/agent1/agent
  test_agent: 
    type: agent/master/selective
    agent1: 
      type: agent/sub/compartmentalized
      min: experiment/agent/agent1/min
      max: experiment/agent/agent1/max
      agent: 
        type: agent/fixed
        policy: 
          type: mapping/policy/action
          sigma: []
          output_min: experiment/environment/task/action_min
          output_max: experiment/environment/task/action_max
          projector: experiment/agent/agent1/agent/policy/policy/projector
          representation: experiment/agent/agent1/agent/policy/policy/representation
    agent2: 
      type: agent/sub/compartmentalized
      min: experiment/agent/agent2/min
      max: experiment/agent/agent2/max
      agent: experiment/test_agent/agent1/agent
  load_file: rbdl_2dpg_ou_squat_fb_sl_vc-run0
  save_every: never
